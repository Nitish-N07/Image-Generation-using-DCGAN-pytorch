{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P9av5SNtm4r"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOLrUY3XMjYo",
        "outputId": "2732be42-269b-4013-f1f6-73e47da4cfc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj5BvFN9MoGw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyP-9VnQtm4t"
      },
      "source": [
        "## Some imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R098oQTS_TC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from torchvision import models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9Ut17hwUDq-"
      },
      "source": [
        "Set the dataset folder, batch size, number of classes, and domain name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4e--IIRU68M"
      },
      "outputs": [],
      "source": [
        "data_folder = 'experiments'\n",
        "batch_size = 16\n",
        "n_class = 5\n",
        "domain_src, domain_tar = 'cartoonV2', 'coralV2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA6e2YaPtm4u"
      },
      "source": [
        "## Data load\n",
        "Now, define a data loader function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JtN_bK9VFcM"
      },
      "outputs": [],
      "source": [
        "def load_data(root_path, domain, batch_size, phase):\n",
        "    transform_dict = {\n",
        "        'src': transforms.Compose(\n",
        "        [transforms.RandomResizedCrop(224),\n",
        "         transforms.RandomHorizontalFlip(),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                              std=[0.229, 0.224, 0.225]),\n",
        "         ]),\n",
        "        'tar': transforms.Compose(\n",
        "        [transforms.Resize(224),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                              std=[0.229, 0.224, 0.225]),\n",
        "         ])}\n",
        "    data = datasets.ImageFolder(root=os.path.join(root_path, domain), transform=transform_dict[phase])\n",
        "    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=phase=='src', drop_last=phase=='tar', num_workers=4)\n",
        "    return data_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHy09lD9tm4v"
      },
      "source": [
        "Load the data using the above function to test it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf_Gw2HRVJM_",
        "outputId": "60e3e696-d882-4922-9304-1675ad8c7be3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source data number: 2285\n",
            "Target data number: 689\n"
          ]
        }
      ],
      "source": [
        "src_loader = load_data(data_folder, domain_src, batch_size, phase='src')\n",
        "tar_loader = load_data(data_folder, domain_tar, batch_size, phase='tar')\n",
        "print(f'Source data number: {len(src_loader.dataset)}')\n",
        "print(f'Target data number: {len(tar_loader.dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caKMn438tm4v"
      },
      "source": [
        "## Define the finetune model\n",
        "The model for finetune is based on ResNet-50 for its popularity. Of course you can use other base networks.\n",
        "The main logic of this class is to get the pretrained ResNet-50, use all of its layers but the last one, which we will replace by a new FC layer for classification. Since the original ResNet-50 is for 1000 classes classification, we only need it to classify 31."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAqT-0jRtm4w"
      },
      "source": [
        "Now, we define a model and test it using a random tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_153Pw1THTOz"
      },
      "outputs": [],
      "source": [
        "# class TransferModel(nn.Module):\n",
        "#     def __init__(self,\n",
        "#                 base_model : str = 'resnet50',\n",
        "#                 pretrain : bool = True,\n",
        "#                 n_class : int = 31):\n",
        "#         super(TransferModel, self).__init__()\n",
        "#         self.base_model = base_model\n",
        "#         self.pretrain = pretrain\n",
        "#         self.n_class = n_class\n",
        "#         if self.base_model == 'resnet50':\n",
        "#             self.model = torchvision.models.resnet50(pretrained=True)\n",
        "#             n_features = self.model.fc.in_features\n",
        "#             fc = torch.nn.Linear(n_features, n_class)\n",
        "#             self.model.fc = fc\n",
        "#         else:\n",
        "#             # Use other models you like, such as vgg or alexnet\n",
        "#             pass\n",
        "#         self.model.fc.weight.data.normal_(0, 0.005)\n",
        "#         self.model.fc.bias.data.fill_(0.1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.model(x)\n",
        "\n",
        "#     def predict(self, x):\n",
        "#         return self.forward(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1KTPUy3Jlst"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models\n",
        "\n",
        "class TransferModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                base_model : str = 'mobilenet_v3_small',\n",
        "                pretrain : bool = True,\n",
        "                n_class : int = 5):\n",
        "        super(TransferModel, self).__init__()\n",
        "\n",
        "        self.base_model = base_model\n",
        "        self.pretrain = pretrain\n",
        "        self.n_class = n_class\n",
        "\n",
        "        if self.base_model == 'resnet50':\n",
        "            self.model = torchvision.models.resnet50(pretrained=self.pretrain)\n",
        "            n_features = self.model.fc.in_features\n",
        "            self.model.fc = nn.Linear(n_features, self.n_class)\n",
        "\n",
        "        elif base_model == 'vgg16':\n",
        "            self.model = models.vgg16(pretrained=pretrain)\n",
        "            num_ftrs = self.model.classifier[6].in_features\n",
        "            self.model.classifier[6] = nn.Linear(num_ftrs, n_class)\n",
        "\n",
        "        elif base_model == 'vgg19':\n",
        "            self.model = models.vgg19(pretrained=pretrain)\n",
        "            num_ftrs = self.model.classifier[6].in_features\n",
        "            self.model.classifier[6] = nn.Linear(num_ftrs, n_class)\n",
        "\n",
        "        elif base_model == 'alexnet':\n",
        "            self.model = models.alexnet(pretrained=pretrain)\n",
        "            num_ftrs = self.model.classifier[6].in_features\n",
        "            self.model.classifier[6] = nn.Linear(num_ftrs, n_class)\n",
        "\n",
        "        elif base_model == 'googlenet':\n",
        "            self.model = models.googlenet(pretrained=pretrain)\n",
        "            num_ftrs = self.model.fc.in_features\n",
        "            self.model.fc = nn.Linear(num_ftrs, n_class)\n",
        "\n",
        "        elif base_model == 'densenet121':\n",
        "            self.model = models.densenet121(pretrained=pretrain)\n",
        "            num_ftrs = self.model.classifier.in_features\n",
        "            self.model.classifier = nn.Linear(num_ftrs, n_class)\n",
        "\n",
        "        elif base_model == 'efficientnet_b0':\n",
        "            # EfficientNet is not natively supported in torchvision as of my last update.\n",
        "            # It will require third-party libraries (e.g., timm). If you have such a library,\n",
        "            # incorporate it accordingly.\n",
        "            pass\n",
        "\n",
        "        elif base_model == 'mnasnet1_0':\n",
        "            self.model = models.mnasnet1_0(pretrained=pretrain)\n",
        "            num_ftrs = self.model.classifier[1].in_features\n",
        "            self.model.classifier[1] = nn.Linear(num_ftrs, n_class)\n",
        "\n",
        "        elif base_model == 'mobilenet_v2':\n",
        "            self.model = models.mobilenet_v2(pretrained=pretrain)\n",
        "            num_ftrs = self.model.classifier[1].in_features\n",
        "            self.model.classifier[1] = nn.Linear(num_ftrs, n_class)\n",
        "\n",
        "        elif base_model == 'mobilenet_v3_small':\n",
        "            self.model = models.mobilenet_v3_small(pretrained=pretrain)\n",
        "            num_ftrs = self.model.classifier[3].in_features\n",
        "            self.model.classifier[3] = nn.Linear(num_ftrs, n_class)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Model {self.base_model} not recognized!\")\n",
        "\n",
        "        if hasattr(self.model, 'fc'):\n",
        "            self.model.fc.weight.data.normal_(0, 0.005)\n",
        "            self.model.fc.bias.data.fill_(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.forward(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEO4SnzfHUib",
        "outputId": "ece74cbe-765f-4f8f-8f76-7d05d1796501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n",
            "100%|██████████| 9.83M/9.83M [00:00<00:00, 53.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1181, -0.0701, -0.0524,  0.1210, -0.0301]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "torch.Size([1, 5])\n"
          ]
        }
      ],
      "source": [
        "model = TransferModel().cuda()\n",
        "RAND_TENSOR = torch.randn(1, 3, 224, 224).cuda()\n",
        "output = model(RAND_TENSOR)\n",
        "print(output)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpX-fuiXtm4w"
      },
      "source": [
        "## Finetune ResNet-50\n",
        "Define some variables. Note that Office-31 doesn't have a validation set, so we use its target domain as the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uNeAiPatm4w"
      },
      "source": [
        "Now the most important part: write the finetune function.\n",
        "This function is pretty easy: it is basically a standard classification function. We train it on the 'src' domain, valid it on the 'val' domain, and then test it on the 'tar' domain.\n",
        "The only difference is that Office-31 dataset has no validation set, so we will use the target domain as the validation set. For your own data, you should use its standard validation set.\n",
        "We also set an early_stop variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h74gKIVqtm4w"
      },
      "outputs": [],
      "source": [
        "dataloaders = {'src': src_loader,\n",
        "               'val': tar_loader,\n",
        "               'tar': tar_loader}\n",
        "n_epoch = 50\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "early_stop = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft9EBLZBHi_m"
      },
      "outputs": [],
      "source": [
        "def finetune(model, dataloaders, optimizer):\n",
        "    since = time.time()\n",
        "    best_acc = 0\n",
        "    stop = 0\n",
        "    for epoch in range(0, n_epoch):\n",
        "        stop += 1\n",
        "        # You can uncomment this line for scheduling learning rate\n",
        "        # lr_schedule(optimizer, epoch)\n",
        "        for phase in ['src', 'val', 'tar']:\n",
        "            if phase == 'src':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            total_loss, correct = 0, 0\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(phase == 'src'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                preds = torch.max(outputs, 1)[1]\n",
        "                if phase == 'src':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                total_loss += loss.item() * inputs.size(0)\n",
        "                correct += torch.sum(preds == labels.data)\n",
        "            epoch_loss = total_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = correct.double() / len(dataloaders[phase].dataset)\n",
        "            print(f'Epoch: [{epoch:02d}/{n_epoch:02d}]---{phase}, loss: {epoch_loss:.6f}, acc: {epoch_acc:.4f}')\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                stop = 0\n",
        "                best_acc = epoch_acc\n",
        "                torch.save(model.state_dict(), 'model.pkl')\n",
        "        if stop >= early_stop:\n",
        "            break\n",
        "        print()\n",
        "\n",
        "    time_pass = time.time() - since\n",
        "    print(f'Training complete in {time_pass // 60:.0f}m {time_pass % 60:.0f}s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGUZGrm7tm4x"
      },
      "source": [
        "Now, define some train parameters and the optimizer. For simplicity, we use SGD, and the learning rate for the FC layer is 10 times of other layers, which is a common trick."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3ysdVe6HY8Q"
      },
      "outputs": [],
      "source": [
        "param_group = []\n",
        "learning_rate = 0.001\n",
        "momentum = 5e-4\n",
        "for k, v in model.named_parameters():\n",
        "    if not k.__contains__('fc'):\n",
        "        param_group += [{'params': v, 'lr': learning_rate}]\n",
        "    else:\n",
        "        param_group += [{'params': v, 'lr': learning_rate * 10}]\n",
        "optimizer = torch.optim.SGD(param_group, momentum=momentum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7zGR_PFtm4y"
      },
      "source": [
        "## Train and test\n",
        "Now we can train and test it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "v9sKSsCyHqu-",
        "outputId": "76491af8-e604-49a4-f8c1-90a091eeb23c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ddb23285f0f4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinetune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-5bdf8bf39f0a>\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(model, dataloaders, optimizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'src'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LAZY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "finetune(model, dataloaders, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR5Y1x4btm4y"
      },
      "outputs": [],
      "source": [
        "def test(model, target_test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    len_target_dataset = len(target_test_loader.dataset)\n",
        "    with torch.no_grad():\n",
        "        for data, target in target_test_loader:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            s_output = model.predict(data)\n",
        "            pred = torch.max(s_output, 1)[1]\n",
        "            correct += torch.sum(pred == target)\n",
        "    acc = correct.double() / len(target_test_loader.dataset)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyDdeThdHsb8",
        "outputId": "d9b03e25-66c7-4a3b-b508-d6a028a25f1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.783744557329463\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('model.pkl'))\n",
        "acc_test = test(model, dataloaders['tar'])\n",
        "print(f'Test accuracy: {acc_test}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO4c_QcGtm4z"
      },
      "source": [
        "## Domain adaptation\n",
        "Now we are in domain adaptation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy_1xwdJtm40"
      },
      "source": [
        "### Loss function\n",
        "The most popular loss function for DA is **MMD (Maximum Mean Discrepancy)**. For comaprison, we also use another popular loss **CORAL (CORrelation ALignment)**. They are defined as follows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3-wKorUtm40"
      },
      "source": [
        "#### MMD loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpQH6VFwtm41"
      },
      "outputs": [],
      "source": [
        "class MMD_loss(nn.Module):\n",
        "    def __init__(self, kernel_type='rbf', kernel_mul=2.0, kernel_num=5):\n",
        "        super(MMD_loss, self).__init__()\n",
        "        self.kernel_num = kernel_num\n",
        "        self.kernel_mul = kernel_mul\n",
        "        self.fix_sigma = None\n",
        "        self.kernel_type = kernel_type\n",
        "\n",
        "    def guassian_kernel(self, source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
        "        n_samples = int(source.size()[0]) + int(target.size()[0])\n",
        "        total = torch.cat([source, target], dim=0)\n",
        "        total0 = total.unsqueeze(0).expand(\n",
        "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
        "        total1 = total.unsqueeze(1).expand(\n",
        "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
        "        L2_distance = ((total0-total1)**2).sum(2)\n",
        "        if fix_sigma:\n",
        "            bandwidth = fix_sigma\n",
        "        else:\n",
        "            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
        "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
        "        bandwidth_list = [bandwidth * (kernel_mul**i)\n",
        "                          for i in range(kernel_num)]\n",
        "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)\n",
        "                      for bandwidth_temp in bandwidth_list]\n",
        "        return sum(kernel_val)\n",
        "\n",
        "    def linear_mmd2(self, f_of_X, f_of_Y):\n",
        "        loss = 0.0\n",
        "        delta = f_of_X.float().mean(0) - f_of_Y.float().mean(0)\n",
        "        loss = delta.dot(delta.T)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        if self.kernel_type == 'linear':\n",
        "            return self.linear_mmd2(source, target)\n",
        "        elif self.kernel_type == 'rbf':\n",
        "            batch_size = int(source.size()[0])\n",
        "            kernels = self.guassian_kernel(\n",
        "                source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
        "            XX = torch.mean(kernels[:batch_size, :batch_size])\n",
        "            YY = torch.mean(kernels[batch_size:, batch_size:])\n",
        "            XY = torch.mean(kernels[:batch_size, batch_size:])\n",
        "            YX = torch.mean(kernels[batch_size:, :batch_size])\n",
        "            loss = torch.mean(XX + YY - XY - YX)\n",
        "            return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcfUy_2Dtm41"
      },
      "source": [
        "#### CORAL loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZhKJq15tm41"
      },
      "outputs": [],
      "source": [
        "def CORAL(source, target):\n",
        "    d = source.size(1)\n",
        "    ns, nt = source.size(0), target.size(0)\n",
        "\n",
        "    # source covariance\n",
        "    tmp_s = torch.ones((1, ns)).cuda() @ source\n",
        "    cs = (source.t() @ source - (tmp_s.t() @ tmp_s) / ns) / (ns - 1)\n",
        "\n",
        "    # target covariance\n",
        "    tmp_t = torch.ones((1, nt)).cuda() @ target\n",
        "    ct = (target.t() @ target - (tmp_t.t() @ tmp_t) / nt) / (nt - 1)\n",
        "\n",
        "    # frobenius norm\n",
        "    loss = (cs - ct).pow(2).sum().sqrt()\n",
        "    loss = loss / (4 * d * d)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB2cDp8Gtm41"
      },
      "source": [
        "### Model\n",
        "Now we use ResNet-50 again just like finetune. The difference is that we rewrite the ResNet-50 class to drop its last layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOLx_OSxtm41"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "class ResNet50Fc(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet50Fc, self).__init__()\n",
        "        model_resnet50 = models.resnet50(pretrained=True)\n",
        "        self.conv1 = model_resnet50.conv1\n",
        "        self.bn1 = model_resnet50.bn1\n",
        "        self.relu = model_resnet50.relu\n",
        "        self.maxpool = model_resnet50.maxpool\n",
        "        self.layer1 = model_resnet50.layer1\n",
        "        self.layer2 = model_resnet50.layer2\n",
        "        self.layer3 = model_resnet50.layer3\n",
        "        self.layer4 = model_resnet50.layer4\n",
        "        self.avgpool = model_resnet50.avgpool\n",
        "        self.__in_features = model_resnet50.fc.in_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "    def output_num(self):\n",
        "        return self.__in_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHUqYLvczQs9"
      },
      "outputs": [],
      "source": [
        "class VggNetFC(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VggNetFC, self).__init__()\n",
        "        model_vgg = models.vgg16(pretrained=True)\n",
        "        self.features = model_vgg.features\n",
        "        self.classifier = nn.Sequential()\n",
        "        for i in range(6):\n",
        "            self.classifier.add_module(\n",
        "                \"classifier\"+str(i), model_vgg.classifier[i])\n",
        "        self.__in_features = model_vgg.classifier[6].in_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "    def output_num(self):\n",
        "        return self.__in_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HzQgbjvVGdx"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import googlenet\n",
        "\n",
        "class GoogLeNetFc(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GoogLeNetFc, self).__init__()\n",
        "        model_googlenet = googlenet(pretrained=True)\n",
        "        # Remove the last classification layer (fc)\n",
        "        self.features = nn.Sequential(*list(model_googlenet.children())[:-1])\n",
        "        self.__in_features = model_googlenet.fc.in_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "    def output_num(self):\n",
        "        return self.__in_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyQ7KFsoCvWt"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import densenet121\n",
        "\n",
        "class DenseNet121Fc(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNet121Fc, self).__init__()\n",
        "        model_densenet121 = densenet121(pretrained=True)\n",
        "        # Remove the last classification layer (classifier)\n",
        "        self.features = model_densenet121.features\n",
        "        self.__in_features = model_densenet121.classifier.in_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "    def output_num(self):\n",
        "        return self.__in_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42JTP914DHni"
      },
      "outputs": [],
      "source": [
        "# import timm\n",
        "\n",
        "class EfficientNetB0Fc(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EfficientNetB0Fc, self).__init__()\n",
        "        model_effnetb0 = timm.create_model('efficientnet_b0', pretrained=True)\n",
        "        # Remove the last classification layer (classifier)\n",
        "        self.features = nn.Sequential(*list(model_effnetb0.children())[:-1])\n",
        "        self.__in_features = model_effnetb0.classifier.in_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "    def output_num(self):\n",
        "        return self.__in_features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYO07_HVHWhl"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import mnasnet1_0\n",
        "\n",
        "class MNASNet1_0Fc(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNASNet1_0Fc, self).__init__()\n",
        "        model_mnasnet = mnasnet1_0(pretrained=True)\n",
        "        # Remove the last classification layer (classifier)\n",
        "        self.features = nn.Sequential(*list(model_mnasnet.children())[:-1])\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.__in_features = model_mnasnet.classifier[1].in_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.adaptive_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "    def output_num(self):\n",
        "        return self.__in_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUbI63oGHgBu"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import mobilenet_v2, mobilenet_v3_small\n",
        "\n",
        "\n",
        "\n",
        "class MobileNetV2Fc(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MobileNetV2Fc, self).__init__()\n",
        "        model_mobilenet_v2 = mobilenet_v2(pretrained=True)\n",
        "        # Remove the last classification layer (classifier)\n",
        "        self.features = model_mobilenet_v2.features\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.__in_features = model_mobilenet_v2.classifier[1].in_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.adaptive_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "    def output_num(self):\n",
        "        return self.__in_features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from torchvision.models import mobilenet_v3_small\n",
        "\n",
        "class MobileNetV3SmallFc(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MobileNetV3SmallFc, self).__init__()\n",
        "        model_mobilenet_v3_small = mobilenet_v3_small(pretrained=True)\n",
        "        # Remove the last classification layer (classifier)\n",
        "        self.features = model_mobilenet_v3_small.features\n",
        "        self.__in_features = 576  # Output channels from the features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        # Global Average Pooling (GAP) layer\n",
        "        x = x.mean([2, 3])\n",
        "        return x\n",
        "\n",
        "    def output_num(self):\n",
        "        return self.__in_features\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2r37jSyKCkO"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import shufflenet_v2_x0_5\n",
        "\n",
        "class ShuffleNetV2_x0_5Fc(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ShuffleNetV2_x0_5Fc, self).__init__()\n",
        "        model_shufflenet = shufflenet_v2_x0_5(pretrained=True)\n",
        "\n",
        "        # Extract all layers except the last fully connected layer\n",
        "        self.features = nn.Sequential(*list(model_shufflenet.children())[:-1])\n",
        "        self.__in_features = model_shufflenet.fc.in_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.mean([2, 3])\n",
        "        return x\n",
        "\n",
        "    def output_num(self):\n",
        "        return self.__in_features\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6CbLACoKe9_"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import alexnet\n",
        "\n",
        "class AlexNetFc(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AlexNetFc, self).__init__()\n",
        "        model_alexnet = alexnet(pretrained=True)\n",
        "        # Remove the last classification layer (classifier)\n",
        "        self.features = model_alexnet.features\n",
        "        self.avgpool = model_alexnet.avgpool\n",
        "        self.__in_features = 9216  # Adjusted to match the output shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "    def output_num(self):\n",
        "        return self.__in_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Sx24aNYKf2l"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import vgg16\n",
        "\n",
        "class VGG16Fc(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16Fc, self).__init__()\n",
        "        model_vgg16 = vgg16(pretrained=True)\n",
        "        # Remove the last classification layer (classifier)\n",
        "        self.features = model_vgg16.features\n",
        "        self.avgpool = model_vgg16.avgpool\n",
        "        self.__in_features = model_vgg16.classifier[6].in_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "    def output_num(self):\n",
        "        return self.__in_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZFGhA8PKhyy"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import vgg19\n",
        "\n",
        "class VGG19Fc(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG19Fc, self).__init__()\n",
        "        model_vgg19 = vgg19(pretrained=True)\n",
        "        # Remove the last classification layer (classifier)\n",
        "        self.features = model_vgg19.features\n",
        "        self.avgpool = model_vgg19.avgpool\n",
        "        self.__in_features = 25088  # Adjusted to match the output shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "    def output_num(self):\n",
        "        return self.__in_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRgdNM3Wtm42"
      },
      "source": [
        "Now the main class for DA. We take ResNet-50 as its backbone, add a bottleneck layer and our own FC layer for classification.\n",
        "Note the `adapt_loss` function. It is just using our predefined MMD or CORAL loss. Of course you can use your own loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC5NKJpJtm42"
      },
      "outputs": [],
      "source": [
        "class TransferNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_class,\n",
        "                 base_net='resnet50',\n",
        "                 transfer_loss='coral',\n",
        "                 use_bottleneck=True,\n",
        "                 bottleneck_width=256,\n",
        "                 width=1024):\n",
        "        super(TransferNet, self).__init__()\n",
        "        if base_net == 'resnet50':\n",
        "            self.base_network = ResNet50Fc()\n",
        "        elif base_net == 'googlenet':\n",
        "            self.base_network = GoogLeNetFc()\n",
        "        elif base_net == 'vgg16':\n",
        "            self.base_network = VggNetFC()\n",
        "        elif base_net == 'alexnet':\n",
        "            self.base_network = AlexNetFc()\n",
        "        elif base_net == 'vgg16_2':\n",
        "            self.base_network = VGG16Fc()\n",
        "        elif base_net == 'vgg19':\n",
        "            self.base_network = VGG19Fc()\n",
        "        elif base_net == 'densenet121':\n",
        "            self.base_network = DenseNet121Fc()\n",
        "        elif base_net == 'efficientnet_b0':\n",
        "            self.base_network = EfficientNetB0Fc()\n",
        "        elif base_net == 'mnasnet1_0':\n",
        "            self.base_network = MNASNet1_0Fc()\n",
        "        elif base_net == 'mobilenet_v2':\n",
        "            self.base_network = MobileNetV2Fc()\n",
        "        elif base_net == 'mobilenet_v3_small':\n",
        "            self.base_network = MobileNetV3SmallFc()\n",
        "        elif base_net == 'shufflenet_v2_x0_5':\n",
        "            self.base_network = ShuffleNetV2_x0_5Fc()\n",
        "        else:\n",
        "            raise ValueError('Unknown base network: {}'.format(base_net))\n",
        "        self.use_bottleneck = use_bottleneck\n",
        "        self.transfer_loss = transfer_loss\n",
        "        bottleneck_list = [nn.Linear(self.base_network.output_num(\n",
        "        ), bottleneck_width), nn.BatchNorm1d(bottleneck_width), nn.ReLU(), nn.Dropout(0.5)]\n",
        "        self.bottleneck_layer = nn.Sequential(*bottleneck_list)\n",
        "        classifier_layer_list = [nn.Linear(self.base_network.output_num(), width), nn.ReLU(), nn.Dropout(0.5),\n",
        "                                 nn.Linear(width, num_class)]\n",
        "        self.classifier_layer = nn.Sequential(*classifier_layer_list)\n",
        "\n",
        "        self.bottleneck_layer[0].weight.data.normal_(0, 0.005)\n",
        "        self.bottleneck_layer[0].bias.data.fill_(0.1)\n",
        "        for i in range(2):\n",
        "            self.classifier_layer[i * 3].weight.data.normal_(0, 0.01)\n",
        "            self.classifier_layer[i * 3].bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        source = self.base_network(source)\n",
        "        target = self.base_network(target)\n",
        "        source_clf = self.classifier_layer(source)\n",
        "        if self.use_bottleneck:\n",
        "            source = self.bottleneck_layer(source)\n",
        "            target = self.bottleneck_layer(target)\n",
        "        transfer_loss = self.adapt_loss(source, target, self.transfer_loss)\n",
        "        return source_clf, transfer_loss\n",
        "\n",
        "    def predict(self, x):\n",
        "        features = self.base_network(x)\n",
        "        clf = self.classifier_layer(features)\n",
        "        return clf\n",
        "\n",
        "    def adapt_loss(self, X, Y, adapt_loss):\n",
        "        \"\"\"Compute adaptation loss, currently we support mmd and coral\n",
        "\n",
        "        Arguments:\n",
        "            X {tensor} -- source matrix\n",
        "            Y {tensor} -- target matrix\n",
        "            adapt_loss {string} -- loss type, 'mmd' or 'coral'. You can add your own loss\n",
        "\n",
        "        Returns:\n",
        "            [tensor] -- adaptation loss tensor\n",
        "        \"\"\"\n",
        "        if adapt_loss == 'mmd':\n",
        "            mmd_loss = MMD_loss()\n",
        "            loss = mmd_loss(X, Y)\n",
        "        elif adapt_loss == 'coral':\n",
        "            loss = CORAL(X, Y)\n",
        "        else:\n",
        "            # Your own loss\n",
        "            loss = 0\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAdPs27btm42"
      },
      "source": [
        "### Train\n",
        "Now the train part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK6P8uMDtm42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d08c54f0-23bf-43ce-9e89-aef59e0fb8f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 269MB/s]\n"
          ]
        }
      ],
      "source": [
        "transfer_loss = 'mmd'\n",
        "learning_rate = 0.0001\n",
        "transfer_model = TransferNet(n_class, transfer_loss=transfer_loss, base_net='resnet50').cuda()\n",
        "optimizer = torch.optim.SGD([\n",
        "    {'params': transfer_model.base_network.parameters()},\n",
        "    {'params': transfer_model.bottleneck_layer.parameters(), 'lr': 10 * learning_rate},\n",
        "    {'params': transfer_model.classifier_layer.parameters(), 'lr': 10 * learning_rate},\n",
        "], lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
        "lamb = 1 # weight for transfer loss, it is a hyperparameter that needs to be tuned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WpUfcHItm42"
      },
      "source": [
        "The main train function. Since we have to enumerate all source and target samples, we have to use `zip` operation to enumerate each pair of these two domains. It is common that two domains have different sizes, but we think by randomly sampling them in many epochs, we may sample each one of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGlVNI2ktm42"
      },
      "outputs": [],
      "source": [
        "def train(dataloaders, model, optimizer):\n",
        "    source_loader, target_train_loader, target_test_loader = dataloaders['src'], dataloaders['val'], dataloaders['tar']\n",
        "    len_source_loader = len(source_loader)\n",
        "    len_target_loader = len(target_train_loader)\n",
        "    best_acc = 0\n",
        "    stop = 0\n",
        "    n_batch = min(len_source_loader, len_target_loader)\n",
        "    for e in range(n_epoch):\n",
        "        stop += 1\n",
        "        train_loss_clf, train_loss_transfer, train_loss_total = 0, 0, 0\n",
        "        model.train()\n",
        "        for (src, tar) in zip(source_loader, target_train_loader):\n",
        "            data_source, label_source = src\n",
        "            data_target, _ = tar\n",
        "            data_source, label_source = data_source.cuda(), label_source.cuda()\n",
        "            data_target = data_target.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            label_source_pred, transfer_loss = model(data_source, data_target)\n",
        "            clf_loss = criterion(label_source_pred, label_source)\n",
        "            loss = clf_loss + lamb * transfer_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss_clf = clf_loss.detach().item() + train_loss_clf\n",
        "            train_loss_transfer = transfer_loss.detach().item() + train_loss_transfer\n",
        "            train_loss_total = loss.detach().item() + train_loss_total\n",
        "        acc = test(model, target_test_loader)\n",
        "        print(f'Epoch: [{e:2d}/{n_epoch}], cls_loss: {train_loss_clf/n_batch:.4f}, transfer_loss: {train_loss_transfer/n_batch:.4f}, total_Loss: {train_loss_total/n_batch:.4f}, acc: {acc:.4f}')\n",
        "        if best_acc < acc:\n",
        "            best_acc = acc\n",
        "            torch.save(model.state_dict(), 'trans_model.pkl')\n",
        "            stop = 0\n",
        "        if stop >= early_stop:\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqSCG6-Xtm43",
        "outputId": "b9f8e788-4d65-42b5-e632-e8bec3150334",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [ 0/50], cls_loss: 1.5280, transfer_loss: 0.3206, total_Loss: 1.8485, acc: 0.5210\n",
            "Epoch: [ 1/50], cls_loss: 1.1735, transfer_loss: 0.3237, total_Loss: 1.4972, acc: 0.8287\n",
            "Epoch: [ 2/50], cls_loss: 0.7882, transfer_loss: 0.3237, total_Loss: 1.1120, acc: 0.8824\n",
            "Epoch: [ 3/50], cls_loss: 0.4830, transfer_loss: 0.3210, total_Loss: 0.8040, acc: 0.8970\n",
            "Epoch: [ 4/50], cls_loss: 0.3803, transfer_loss: 0.3245, total_Loss: 0.7048, acc: 0.8868\n",
            "Epoch: [ 5/50], cls_loss: 0.2951, transfer_loss: 0.3257, total_Loss: 0.6208, acc: 0.9115\n",
            "Epoch: [ 6/50], cls_loss: 0.2506, transfer_loss: 0.3216, total_Loss: 0.5722, acc: 0.8970\n",
            "Epoch: [ 7/50], cls_loss: 0.2207, transfer_loss: 0.3205, total_Loss: 0.5412, acc: 0.9086\n",
            "Epoch: [ 8/50], cls_loss: 0.2280, transfer_loss: 0.3178, total_Loss: 0.5458, acc: 0.9071\n",
            "Epoch: [ 9/50], cls_loss: 0.2070, transfer_loss: 0.3161, total_Loss: 0.5232, acc: 0.8970\n",
            "Epoch: [10/50], cls_loss: 0.1310, transfer_loss: 0.3098, total_Loss: 0.4408, acc: 0.8926\n",
            "Epoch: [11/50], cls_loss: 0.1437, transfer_loss: 0.3058, total_Loss: 0.4495, acc: 0.8781\n",
            "Epoch: [12/50], cls_loss: 0.1425, transfer_loss: 0.2925, total_Loss: 0.4349, acc: 0.8984\n",
            "Epoch: [13/50], cls_loss: 0.1307, transfer_loss: 0.2738, total_Loss: 0.4045, acc: 0.8781\n",
            "Epoch: [14/50], cls_loss: 0.1311, transfer_loss: 0.2573, total_Loss: 0.3884, acc: 0.8665\n",
            "Epoch: [15/50], cls_loss: 0.0941, transfer_loss: 0.2373, total_Loss: 0.3313, acc: 0.8984\n",
            "Epoch: [16/50], cls_loss: 0.1391, transfer_loss: 0.2207, total_Loss: 0.3598, acc: 0.8795\n",
            "Epoch: [17/50], cls_loss: 0.1139, transfer_loss: 0.2110, total_Loss: 0.3248, acc: 0.8999\n",
            "Epoch: [18/50], cls_loss: 0.1347, transfer_loss: 0.1978, total_Loss: 0.3326, acc: 0.9013\n",
            "Epoch: [19/50], cls_loss: 0.1171, transfer_loss: 0.1927, total_Loss: 0.3098, acc: 0.8766\n",
            "Epoch: [20/50], cls_loss: 0.1428, transfer_loss: 0.1832, total_Loss: 0.3260, acc: 0.8708\n",
            "Epoch: [21/50], cls_loss: 0.0929, transfer_loss: 0.1800, total_Loss: 0.2729, acc: 0.8766\n",
            "Epoch: [22/50], cls_loss: 0.0914, transfer_loss: 0.1715, total_Loss: 0.2629, acc: 0.8476\n",
            "Epoch: [23/50], cls_loss: 0.1201, transfer_loss: 0.1665, total_Loss: 0.2866, acc: 0.8955\n",
            "Epoch: [24/50], cls_loss: 0.0977, transfer_loss: 0.1615, total_Loss: 0.2592, acc: 0.8389\n",
            "Epoch: [25/50], cls_loss: 0.1176, transfer_loss: 0.1575, total_Loss: 0.2751, acc: 0.8926\n"
          ]
        }
      ],
      "source": [
        "train(dataloaders, transfer_model, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWvYODRDtm43",
        "outputId": "2cc9fad2-510d-46cc-cc10-c9be19e2a30b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.7968069666182873\n"
          ]
        }
      ],
      "source": [
        "transfer_model.load_state_dict(torch.load('trans_model.pkl'))\n",
        "acc_test = test(transfer_model, dataloaders['tar'])\n",
        "print(f'Test accuracy: {acc_test}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbw6KndNtm43"
      },
      "source": [
        "Now we are done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQX2ak-jtm43"
      },
      "source": [
        "You see, we don't even need to install a library or package to train a domain adaptation or finetune model.\n",
        "In your own work, you can also use this notebook to test your own algorithms."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}